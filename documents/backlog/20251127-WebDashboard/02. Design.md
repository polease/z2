# Web Dashboard Design - Z2 AI Knowledge Distillery Platform

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                         Browser                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │           React Frontend (SPA)                         │ │
│  │  - Dashboard View                                      │ │
│  │  - Job Submission Form                                 │ │
│  │  - Job History Table                                   │ │
│  │  - Job Detail View                                     │ │
│  │  - Real-time Log Viewer                                │ │
│  └────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
                          ↕ HTTP/REST + WebSocket
┌─────────────────────────────────────────────────────────────┐
│                    FastAPI Backend                          │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  REST API Endpoints                                    │ │
│  │  - POST /api/jobs (submit URL)                         │ │
│  │  - GET /api/jobs (list all jobs)                       │ │
│  │  - GET /api/jobs/{id} (job details)                    │ │
│  │  - DELETE /api/jobs/{id} (cancel job)                  │ │
│  └────────────────────────────────────────────────────────┘ │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  WebSocket Endpoints                                   │ │
│  │  - WS /ws/jobs/{id}/logs (stream logs)                 │ │
│  │  - WS /ws/jobs/status (broadcast status updates)       │ │
│  └────────────────────────────────────────────────────────┘ │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  Background Task Manager                               │ │
│  │  - Job Queue (asyncio queue)                           │ │
│  │  - Worker Pool                                         │ │
│  │  - Pipeline Executor                                   │ │
│  └────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
                          ↕
┌─────────────────────────────────────────────────────────────┐
│                    PostgreSQL Database                       │
│  Tables:                                                     │
│  - jobs (id, url, status, created_at, ...)                  │
│  - job_logs (id, job_id, timestamp, level, message)         │
│  - job_metadata (job_id, video_title, duration, ...)        │
│  - job_files (job_id, file_type, file_path)                 │
└─────────────────────────────────────────────────────────────┘
                          ↕
┌─────────────────────────────────────────────────────────────┐
│              Existing Pipeline Components                    │
│  - YouTubeDownloader                                         │
│  - Transcriber                                               │
│  - Translator                                                │
│  - VideoProcessor                                            │
│  - Publisher                                                 │
└─────────────────────────────────────────────────────────────┘
```

## Database Schema

### Table: `jobs`
```sql
CREATE TABLE jobs (
    id SERIAL PRIMARY KEY,
    job_uuid UUID UNIQUE NOT NULL DEFAULT gen_random_uuid(),
    youtube_url TEXT NOT NULL,
    video_id VARCHAR(20),
    status VARCHAR(50) NOT NULL,
    progress INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,
    cancelled BOOLEAN DEFAULT FALSE
);

CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_created_at ON jobs(created_at DESC);
```

### Table: `job_metadata`
```sql
CREATE TABLE job_metadata (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
    video_title TEXT,
    channel_name TEXT,
    channel_id TEXT,
    upload_date TEXT,
    duration INTEGER,
    view_count INTEGER,
    like_count INTEGER,
    thumbnail_url TEXT,
    description TEXT,
    UNIQUE(job_id)
);
```

### Table: `job_logs`
```sql
CREATE TABLE job_logs (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    level VARCHAR(20),
    message TEXT,
    stage VARCHAR(50)
);

CREATE INDEX idx_job_logs_job_id ON job_logs(job_id, timestamp);
```

### Table: `job_files`
```sql
CREATE TABLE job_files (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
    file_type VARCHAR(50),
    file_path TEXT,
    file_size_mb DECIMAL(10, 2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_job_files_job_id ON job_files(job_id);
```

### Table: `job_analysis`
```sql
CREATE TABLE job_analysis (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
    summary TEXT,
    key_insights JSONB,
    highlights JSONB,
    topics JSONB,
    UNIQUE(job_id)
);
```

### Table: `job_publishing`
```sql
CREATE TABLE job_publishing (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
    platform VARCHAR(50),
    status VARCHAR(50),
    published_at TIMESTAMP,
    post_id TEXT,
    url TEXT,
    error_message TEXT
);

CREATE INDEX idx_job_publishing_job_id ON job_publishing(job_id);
```

## Job Status Flow

```
PENDING → DOWNLOADING → TRANSCRIBING → TRANSLATING →
PROCESSING_VIDEO → PUBLISHING → COMPLETED

                    ↓ (any stage)
                 FAILED

                    ↓ (user action)
                CANCELLED
```

Status values:
- `PENDING`: Job queued, waiting to start
- `DOWNLOADING`: Downloading video from YouTube
- `TRANSCRIBING`: Extracting/generating transcripts
- `TRANSLATING`: Translating to Chinese
- `PROCESSING_VIDEO`: Burning subtitles
- `PUBLISHING`: Publishing to platforms
- `COMPLETED`: All steps successful
- `FAILED`: Error occurred
- `CANCELLED`: User cancelled the job

## API Endpoints

### 1. Submit New Job
```
POST /api/jobs
Content-Type: application/json

Request:
{
  "youtube_url": "https://www.youtube.com/watch?v=..."
}

Response (201):
{
  "job_id": "uuid-here",
  "status": "PENDING",
  "created_at": "2025-11-27T22:30:00Z"
}

Response (400):
{
  "error": "Invalid YouTube URL"
}
```

### 2. List Jobs
```
GET /api/jobs?status=COMPLETED&limit=50&offset=0

Response (200):
{
  "jobs": [
    {
      "job_id": "uuid",
      "youtube_url": "...",
      "video_id": "...",
      "video_title": "...",
      "status": "COMPLETED",
      "progress": 100,
      "created_at": "...",
      "completed_at": "..."
    }
  ],
  "total": 150,
  "limit": 50,
  "offset": 0
}
```

### 3. Get Job Details
```
GET /api/jobs/{job_id}

Response (200):
{
  "job_id": "uuid",
  "youtube_url": "...",
  "video_id": "...",
  "status": "COMPLETED",
  "progress": 100,
  "created_at": "...",
  "started_at": "...",
  "completed_at": "...",
  "metadata": {
    "video_title": "...",
    "channel_name": "...",
    "duration": 1200,
    ...
  },
  "files": [
    {
      "file_type": "original_video",
      "file_path": "...",
      "file_size_mb": 150.5
    }
  ],
  "analysis": {
    "summary": "...",
    "key_insights": [...],
    "topics": [...]
  },
  "publishing": [
    {
      "platform": "wechat",
      "status": "success",
      "url": "..."
    }
  ]
}
```

### 4. Cancel Job
```
DELETE /api/jobs/{job_id}

Response (200):
{
  "message": "Job cancelled",
  "job_id": "uuid"
}

Response (400):
{
  "error": "Job already completed"
}
```

### 5. Get Job Statistics
```
GET /api/stats

Response (200):
{
  "total_jobs": 250,
  "completed": 230,
  "failed": 15,
  "running": 2,
  "pending": 3,
  "avg_duration_minutes": 12.5
}
```

## WebSocket Endpoints

### 1. Job Status Updates (Broadcast)
```
WS /ws/jobs/status

Server broadcasts to all clients:
{
  "job_id": "uuid",
  "status": "TRANSCRIBING",
  "progress": 45,
  "stage": "transcribing",
  "timestamp": "2025-11-27T22:30:00Z"
}
```

### 2. Job Logs Stream
```
WS /ws/jobs/{job_id}/logs

Server sends:
{
  "timestamp": "2025-11-27T22:30:15Z",
  "level": "INFO",
  "stage": "downloading",
  "message": "[download] Downloading video..."
}
```

## Backend Components

### 1. FastAPI Application Structure
```
src/web/
├── __init__.py
├── main.py                 # FastAPI app entry point
├── config.py              # Configuration
├── database.py            # Database connection
├── models.py              # SQLAlchemy models
├── schemas.py             # Pydantic schemas
├── api/
│   ├── __init__.py
│   ├── jobs.py           # Job CRUD endpoints
│   ├── websocket.py      # WebSocket handlers
│   └── stats.py          # Statistics endpoints
├── services/
│   ├── __init__.py
│   ├── job_service.py    # Job business logic
│   ├── pipeline_runner.py # Run pipeline in background
│   └── log_streamer.py   # Stream logs via WebSocket
└── utils/
    ├── __init__.py
    ├── logger_db.py      # Custom logger to DB
    └── validators.py     # URL validation, etc.
```

### 2. Job Queue Manager
```python
# Async job queue with worker pool
class JobQueueManager:
    def __init__(self, max_workers: int = 2):
        self.queue = asyncio.Queue()
        self.workers = []
        self.max_workers = max_workers
        self.active_jobs = {}  # job_id -> task

    async def enqueue(self, job_id: str, youtube_url: str):
        """Add job to queue"""

    async def worker(self):
        """Process jobs from queue"""

    async def cancel_job(self, job_id: str):
        """Cancel running job"""
```

### 3. Pipeline Integration
```python
# Wrapper around existing pipeline
class PipelineRunner:
    def __init__(self, job_id: str, youtube_url: str):
        self.job_id = job_id
        self.youtube_url = youtube_url
        self.cancelled = False

    async def run(self):
        """Run complete pipeline with status updates"""
        # 1. Update status to DOWNLOADING
        # 2. Run downloader
        # 3. Update status to TRANSCRIBING
        # 4. Run transcriber
        # ... etc
        # Stream logs to WebSocket
        # Update database at each stage
```

### 4. Custom Database Logger
```python
# Logger that writes to both file and database
class DatabaseLogger:
    def __init__(self, job_id: str):
        self.job_id = job_id

    def info(self, message: str, stage: str):
        # Write to job_logs table
        # Broadcast via WebSocket
```

## Frontend Components

### React App Structure
```
frontend/
├── public/
│   └── index.html
├── src/
│   ├── App.tsx
│   ├── index.tsx
│   ├── components/
│   │   ├── Dashboard.tsx          # Main dashboard
│   │   ├── JobSubmissionForm.tsx  # URL input form
│   │   ├── JobList.tsx            # Job history table
│   │   ├── JobDetail.tsx          # Detailed job view
│   │   ├── JobStatusBadge.tsx     # Status indicator
│   │   ├── LogViewer.tsx          # Real-time log viewer
│   │   └── ProgressBar.tsx        # Progress indicator
│   ├── hooks/
│   │   ├── useWebSocket.ts        # WebSocket hook
│   │   ├── useJobs.ts             # Job data fetching
│   │   └── useJobLogs.ts          # Log streaming
│   ├── services/
│   │   └── api.ts                 # API client
│   ├── types/
│   │   └── job.ts                 # TypeScript types
│   └── utils/
│       └── formatters.ts          # Date, duration formatters
├── package.json
└── tsconfig.json
```

### Key React Components

#### 1. Dashboard (Main View)
```tsx
<Dashboard>
  <Header>
    <Stats total={250} running={2} completed={230} />
  </Header>
  <JobSubmissionForm onSubmit={handleSubmit} />
  <ActiveJobs jobs={activeJobs} />
  <JobList jobs={allJobs} onSelectJob={setSelectedJob} />
</Dashboard>
```

#### 2. Job Detail Modal
```tsx
<JobDetailModal job={selectedJob}>
  <Tabs>
    <Tab label="Overview">
      <JobMetadata />
      <JobFiles />
    </Tab>
    <Tab label="Logs">
      <LogViewer jobId={job.id} />
    </Tab>
    <Tab label="Analysis">
      <AnalysisResults />
    </Tab>
  </Tabs>
</JobDetailModal>
```

#### 3. Real-time Log Viewer
```tsx
function LogViewer({ jobId }: { jobId: string }) {
  const { logs, connected } = useJobLogs(jobId);

  return (
    <LogContainer autoScroll>
      {logs.map(log => (
        <LogLine key={log.id} level={log.level}>
          [{log.timestamp}] {log.message}
        </LogLine>
      ))}
    </LogContainer>
  );
}
```

## WebSocket Integration

### Client-side WebSocket Hook
```typescript
function useWebSocket(url: string) {
  const [messages, setMessages] = useState([]);
  const [connected, setConnected] = useState(false);

  useEffect(() => {
    const ws = new WebSocket(url);

    ws.onopen = () => setConnected(true);
    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setMessages(prev => [...prev, data]);
    };
    ws.onclose = () => setConnected(false);

    return () => ws.close();
  }, [url]);

  return { messages, connected };
}
```

## Deployment Architecture

```
┌─────────────────────────────────────────┐
│         Nginx (Reverse Proxy)           │
│  - Serve React build (/)                │
│  - Proxy API (/api → FastAPI:8000)      │
│  - Proxy WebSocket (/ws → FastAPI:8000) │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│         FastAPI (Uvicorn)               │
│  - Port 8000                            │
│  - Handle HTTP + WebSocket              │
└─────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────┐
│         PostgreSQL                       │
│  - Port 5432                            │
└─────────────────────────────────────────┘
```

## File Storage Strategy

Keep existing file structure:
```
storage/
├── videos/
│   └── {year-month}/
│       ├── {video_id}.mp4
│       └── {video_id}_zh_subbed.mp4
├── subtitles/
│   └── {year-month}/
│       ├── {video_id}.en.srt
│       └── {video_id}_zh.srt
├── data/              # Keep for backward compatibility
│   └── {year-month}/
│       └── {video_id}.json
└── logs/
    └── app.log
```

Store file paths in database, actual files remain on disk.

## Migration Strategy

1. **Database Setup**
   - Create PostgreSQL database and tables
   - Create migration script to import existing JSON files

2. **Backend Development**
   - Set up FastAPI project structure
   - Implement database models and API endpoints
   - Integrate with existing pipeline code
   - Add WebSocket support

3. **Frontend Development**
   - Set up React project with TypeScript
   - Implement UI components
   - Connect to API and WebSocket

4. **Testing**
   - Test job submission and pipeline execution
   - Test WebSocket connections
   - Test job cancellation
   - Test with concurrent jobs

5. **Deployment**
   - Set up Nginx configuration
   - Configure systemd services
   - Set up PostgreSQL backups

## Security Considerations

For MVP (single user):
- No authentication required
- Frontend and backend on same domain (CORS not needed)
- WebSocket connections allowed from same origin

For future multi-user:
- Add JWT authentication
- Implement user roles
- Add rate limiting
- Add CORS configuration

## Performance Optimization

- Database indexing on frequently queried columns
- Pagination for job list (limit 50 per page)
- WebSocket connection pooling
- Log retention policy (delete old logs after 30 days)
- File cleanup for old jobs (optional)

## Error Handling

- API returns proper HTTP status codes
- WebSocket reconnection on disconnect
- Database transaction rollback on errors
- Pipeline stage rollback on cancellation
- User-friendly error messages in UI

---

**Status**: Design complete, ready for review and implementation.
